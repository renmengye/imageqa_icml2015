\chapter{Introduction}
Vision and language are two major inputs of human perception. Everyday, we see images through eyes and listen to speech through ears, and often the two are associated and complementary to each other. Speakers use visual elements such as projector slides to make their lectures easier to understand, and photographers use captions and descriptions to convey deeper thoughts of their photos. 

However, compared to humans, computers are extremely poor at vision and language. A human can recognize objects in each part of the image and describe them using his/her own words, but it is still a very difficult task for computers. In recent years, computational models, pariticularly large-scale deep convolutional and recurrent neural networks, have achieved eminent success in the field of object recognition and natural language processing \cite{simonyan14, krizhevsky12, sutskever14, kiros14b}. The achievement from previous research has formed the building blocks of learning higher level representations of vision and language.

In this work, we hope to design machine learning models that can jointly learn vision and language through a question-answering (QA) task. The computer is given a set of images and questions regarding the images, and is expected to output correct answers through a supervised learning scheme. Machines need to combine knowledge from both image and words in order to achieve good performance.

Building an image-based question answering system has profound implication in the field of artificial intelligence (AI). Natural language human-computer interation eliminates the need for prerequisite knowledge of operating electronic devices, which will make technologies more accessible. Exchanging image understanding through natural language can also bring benefits to users with visual disabilities. Image-based question answering technology will trigger countless AI applications and can be directly deployed in robots and wearables to assist humans on daily basis.

Tackling the problem is not an easy task, due to multiple layers of complexity. Not only does the model need to parse the scenes and objects in the image, it also needs to understand the question and generate the relevant answer. Compared to image description generation, image QA requires answers to be targeted to the questions with higher precision. 
Image QA is still a fairly new field. The current available datasets were just released last December \cite{malinowski14a}. The authors also presented their approach to this problem, which can be improved by a large margin. In short, there is a lack of large and high-quality datasets as well as competitive models on this problem.

Our goal is to design a machine learning model that learns a representation combining both images and sentences. Unlike the existing approach, we formulate the problem using embedding spaces. First, we need to convert the images and questions into vector forms so that they can be sent as inputs to a neural network. We propose to use the state-of-the-art convolutional neural networks to obtain the visual features, and use recurrent neural networks to obtain sentence-level embedding. We will cover the basics of neural networks and embedding models in Chapter \ref{background}. Now, we can reduce the problem into a multi-class classification problem by assuming only single-word answers. We aim to learn a function that outputs either the answer class or the nearest neighbour of the correct answer in the semantic embedding space. Lastly, as the currently available dataset may be insufficient for our approach, we propose an automatic question generation algorithm to synthesize QA pairs from a large image description dataset.

In conclusion, solving the problem of image-based question answering will be a meaningful step in the field of machine learning and AI. We hope this work will invite more research in the future linking computer vision and natural language knowledge in a broader sense.

\section{Overview of the thesis}

Chapter 2 will introduce the background of our proposed methodology, including the very basics of neural networks and word embeddings as well as more recent research on jointly learning image and text with neural network based embedding models. Chapter 3 will describe our proposed approaches and results. We will explain the models in detail with their performance. We will present an algorithm that helps us collecting a large scale image QA dataset. Chapter 4 will discuss the experimental results in depth, and lastly Chapter 5 will propose future work on our line of research.