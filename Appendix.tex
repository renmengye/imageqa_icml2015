\appendix
\chapter{Neural networks training techniques}
\label{train_tech}
\section{Momentum}
Momentum is a common optimization technique applied on training neural nets. It takes a portion of the last weight update to propagate forward, which significantly speeds up the training by many times. Usually the momentum value is taken between 0.5 and 0.99.

\begin{equation}
\Delta \+w_{t+1} =  - \gamma \left(\dfrac{\del E}{\del \+w}\right)_t  + \alpha \Delta \+w_t
\end{equation}

In the equation above, $\alpha$ is the momentum term.

\section{Weight regularization}
Overfitting occurs when the model performs too well on the training set but generalizes poorly on the test set. Regularization is often applied to prevent overfitting. Common weight regularization methods include square weight loss, absolute weight loss, etc. In the case of square weight loss, we penalize the norm of the weight vector, to prevent each weight element from growing too large. In the backpropagation of the neural networks, the penalty can also be computed in the derivative.

\begin{equation}
\Delta \+w_{t+1} = - \gamma \left(\dfrac{\del E}{\del \+w}\right)_t  + \alpha \Delta \+w_t - \lambda \+w
\end{equation}

$\lambda$ here is defined to be the weight regularization constant.

\section{Dropout}
Another way to prevent overfitting in deep neural networks with a large number of weights is through dropout \cite{srivastava14}. During training, dropout disables a random set of hidden units with certain dropout probability, so the network cannot rely on co-adaptation of hidden activations to learn the desired output. During training time,

\begin{equation}
f(X) = D \odot X, D \sim \text{Bernoulli}(p)
\end{equation}

$D$ is a Bernoulli random vector of zeros and ones, with dropout probability $(1-p)$. $\odot$ denotes component-wise product. During testing, the output is rescaled by the dropout probability to match with training statistics. 

\begin{equation}
f(X) = pX
\end{equation}

\section{Gradient control in training RNNs}
\subsection{Gradient clipping}
Exploding gradient is a common issue in the training of RNNs. Instead of having a uniform learning rate for any gradient update, the gradient clipping method \cite{pascanu13} imposes a hard constraint on the norm of the gradient update. We rescale the gradient if the the norm of the gradient is larger than the constraint.

\begin{equation}
{\del E} / {\del W}_{\text{new}} = \begin{cases}
  {\del E} / {\del W} , & \text{if } \norm{{\del E} / {\del W}} \le C, \\
  {\del E}{\del W} / \norm{{\del E} / {\del W}}, & \text{otherwise}.
\end{cases}
\end{equation}

\subsection{Weight clipping}
Weight clipping is equivalent to adding a hard constraint to the weight vector. It rescales the weight vector whenever its norm surpasses the contraint. This effectively prevents the weights from exploding during the training. 

\begin{equation}
\min E(w)\ \ \ \text{subject to}\ \ \ g(w) = ||w||^2 - U \le 0
\end{equation}

\section{Dropout in RNNs}
Unlike in feedforward networks, dropout in RNNs cannot be simply applied on any hidden units \cite{zaremba14}. In fact, dropout is only shown to be effective if applied on non-recurrent connections, i.e. the input neurons. In case of multiple stacked LSTMs, a dropout rate of 20\% in the first LSTM input, and 50\% in the following inter-stage inputs are shown to work the best \cite{zaremba14}.

\chapter{Model training details}
\section{Image-word model}
\label{sec:imgword}
We list here the hyperparameters of the model by layers.
\begin{enumerate}
\item Image embedding layer\\
Using Oxford net \cite{simonyan14} last hidden layer 4096 dimension features\\
Learning rate: 0.0
\item Word embedding layer\\
Using word2vec skip gram embedding vectors trained from Google News \cite{mikolov13}\\
If the word is not found then initialize uniform from [-0.21, 0.21]\\
Learning rate: 0.0
\item Image linear transformation layer\\
Input dimension: 4096\\
Output dimension: 300\\
Initialization: uniform [-0.05, 0.05]\\
Learning rate: 0.8\\
Momentum: 0.9\\
Gradient clipping: 0.1\\
Weight clipping: 100.0
\item Input dropout layer\\
Dropout rate: 20\%
\item LSTM layer\\
Input dimension: 300\\
Memory dimension: 150\\
Initialization of $\+b_i, \+b_f, \+b_o$: 1.0\\
Initialization of $\+b_c$: 0.0\\
Initialization of other weights: uniform [-0.05, 0.05]\\
Learning rate: 0.8\\
Momentum: 0.9\\
Gradient clipping: 0.1\\
Weight clipping: 100.0\\
Weight regularization: square loss, $\lambda$ = 5e-5
\item Softmax layer\\
Input dimension: 150\\
Initialization: uniform [-0.05, 0.05]\\
Learning rate: 0.01\\
Momentum: 0.9\\
Gradient clipping: 0.1\\
Weight clipping: 10.0 in DAQUAR, 15.0 in COCO-QA 6.6K, 50.0 in COCO-QA Full\\
Weight regularization: square loss, $\lambda$ = 5e-5
\end{enumerate}

\section{Blind model}
All hyperparameters are kept the same with the image-word model, except that we replaced the 4096 dimension frozen image feature input with 4096 dimension frozen random vectors, uniformly initialized from range [-0.05, 0.05].

\section{Image-word ranking model}
Layers before softmax are kept the same with the image-word model, except that the LSTM hidden dimension is now 300 to match with the vector length.\\
Then the cosine similarity layer computes the cosine similarity of the output of the LSTM and the answer vector bank which is directly retrieved from word2vec.\\
The answer that has the largest similarity with the output vector is selected to be the predicted answer.

\section{Bidirectional model}
\label{sec:bimgword}
We list here the hyperparameters of the model by layers.
\begin{enumerate}
\item Image embedding layer\\
Using Oxford net \cite{simonyan14} last hidden layer 4096 dimension features\\
Learning rate: 0.0
\item Word embedding layer\\
Using word2vec skip gram embedding vectors trained from Google News \cite{mikolov13}\\
If the word is not found then initialize uniform from [-0.21, 0.21]\\
Learning rate: 0.0
\item Image linear transformation layer\\
Input dimension: 4096\\
Output dimension: 300\\
Initialization: uniform [-0.05, 0.05]\\
Learning rate: 0.8\\
Momentum: 0.9\\
Gradient clipping: 0.1\\
Weight clipping: 100.0
\item Forward LSTM input dropout layer\\
Dropout rate: 20\%
\item Forward LSTM layer\\
Input dimension: 300\\
Memory dimension: 150\\
Initialization of $\+b_i, \+b_f, \+b_o$: 1.0\\
Initialization of $\+b_c$: 0.0\\
Initialization of other weights: uniform [-0.05, 0.05]\\
Learning rate: 0.8\\
Momentum: 0.9\\
Gradient clipping: 0.1\\
Weight clipping: 100.0\\
Weight regularization: square loss, $\lambda$ = 5e-5
\item Backward LSTM input dropout layer\\
Dropout rate: 50\%
\item Forward-to-backward dropout layer\\
Dropout rate: 50\%
\item Backward LSTM layer\\
Same as forward, except input dimension is now 450 because it takes both the word vector and the output of the forward LSTM.
\item Softmax layer\\
Input dimension: 150\\
Initialization: uniform [-0.05, 0.05]\\
Learning rate: 0.01\\
Momentum: 0.9\\
Gradient clipping: 0.1\\
Weight clipping: 10.0 in DAQUAR, 15.0 in COCO-QA 6.6K, 50.0 in COCO-QA Full\\
Weight regularization: square loss, $\lambda$ = 5e-5
\end{enumerate}