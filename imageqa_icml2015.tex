%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2015 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2015,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
%\usepackage{subfigure}

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% For syntax trees
\usepackage{qtree}
\usepackage{ulem}
\renewcommand{\#}[1]{\textbf{#1}}

% For diagrams
\usepackage{tikz}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2015} with
% \usepackage[nohyperref]{icml2015} above.
\usepackage{hyperref}

\usepackage{footnote}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2015}

% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2015}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
%\icmltitlerunning{Submission and Formatting Instructions for ICML 2015}

\begin{document} 

\twocolumn[
\icmltitle{Question Answering about Images using Visual Semantic Embeddings}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2015
% package.
\icmlauthor{Mengye Ren}{mren@cs.toronto.edu}
\icmlauthor{Ryan Kiros}{rkiros@cs.toronto.edu}
\icmlauthor{Richard Zemel}{zemel@cs.toronto.edu}
\icmladdress{Department of Computer Science, University of Toronto, Toronto, ON, CANADA}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{}

\vskip 0.3in
]

\begin{abstract}
This work aims to address the problem of image-based question-answering (QA) with new models and datasets. In our work, we propose to use recurrent neural networks and visual semantic embeddings without intermediate stages such as object detection and image segmentation. Our model performs 1.7 times better than the recently published results on the same dataset. Another main contribution is an automatic question generation algorithm that converts the currently available image description dataset into QA form, resulting in a 10 times bigger dataset with more even answer distribution.
\end{abstract}

\section{Introduction}
Combining image understanding and natural language interaction is one of the grand dream of artificial intelligence. We are interested in the problem of joinly learning image and text through a question-answering task. Recently, image caption generation \cite{kiros14b,vinyals14,xu15} has shown us feasible ways of jointly learning image and text to form higher level representations  from models such as convolutional neural networks (CNNs) trained on object recognition and word embeddings trained on large scale corpus. Image QA involves an extra layer of interation between human and computers.  Here the input is combined image and text and the model needs to output an answer that is targeted to the question asked. The model needs to pay attention to details of the image instead of describing it in a vague sense. The problem also combines many computer vision subproblems such as image labelling and object detection. Unlike traditional computer vision tasks, it is not obvious what kind of task the model needs to perform unless it understands the question. 

In this paper we present our contribution to the problem: a generic end-to-end QA model using visual semantic embeddings to connect CNN and recurrent neural net (RNN), an automatic question generation algorithm that converts description sentences into questions, and a synthetic QA dataset which is generated using the algorithm. We will make the our synthetic COCO-QA dataset publicly available upon the release of the paper and we invite follow-up research in the community to contribute to this problem.

\section{Problem Formulation}
The input of the problem is a piece of image, a question of a sequence of words, and the output is an answer which is also a sequence of words. In this work, however, we assume that the answers are only single-word, which allows us to treat the problem as a classification problem. The assumption is made based on the fact that 98.3\% of the examples in the recently released dataset \cite{malinowski14a} are single-word. This also makes the evaluation of the models easier and more robust.

\section{Related Work}
In 2014, \cite{malinowski14a} released a dataset with images and question-answer pairs. It is called DAtaset for QUestion Answering on Real-world images (DAQUAR) . All images are from the NYU depth v2 dataset \cite{silberman12}, and are taken from indoor scenes. Human segmentations, image depth values, and object labellings are available in the dataset. The QA data has two sets of configuration: the 37-class and the 894-class dataset, differed by the number of object classes appearing in the questions. There are mainly three types of questions in this dataset: object type, object color, and number of objects. Some questions are easy but many questions are very hard to answer even to humans. Figure~\ref{fig:daquar_easy_hard} shows some examples of easy and hard questions. Since DAQUAR is the only publicly available image-based QA dataset, it is one of our benchmarks to evaluate our models.

Together with the release of DAQUAR dataset, \cite{malinowski14b} presented an approach which combines semantic parsing and image segmentation. In the natural language part of their work, they used a semantic parser \cite{liang13} to convert sentences into latent logical forms. They obtained the multiple segmentations of the image by sampling the uncertainty of the segmentation algorithm. Their model is based on a Bayesian formulation that every logical form and image segmentation has certain probability. They also converted every image segmentation to a bag of predicates. To make their algorithm scalable, they chose to sample from the nearest neighbors in the training set according to the similarity of the predicates.

As there are not much existing work done in the field of image question answering, their attempt have large room for improvement. First, although they are handling a number of spatial relations, a human-defined possible set of predicates are very dataset-specific. To obtain the predicates, their algorithm also depends on a good image segmentation algorithm and image depth information. Second, before asking any of the questions, their model needs to compute all possible spatial relations in the training images, so even for a small dataset like 1500 images there could be 4 millions predicates in the worst case \cite{malinowski14b}. Even though the model searches from the nearest neighbors of the test images, it could still be an expensive operation in larger datasets. Lastly the accuracy of their model is not very strong. We will show later that some simple baselines will perform better in terms of plain accuracy.

\begin{figure*}[ht!]
\centering
\small
$\begin{array}{p{5cm} p{5cm} p{5cm}}
    \scalebox{0.3}{
        \includegraphics[width=\textwidth, height=.7\textwidth]{daquar_img/2355.jpg}}
    \parbox{5cm}{
        \vskip 0.05in
        Q2355: what is the colour of roll of tissue paper ?\\
        Ground truth: white\\
        Easy because toilet tissue paper is always white}
&
    \scalebox{0.3}{
        \includegraphics[width=\textwidth, height=.7\textwidth]{daquar_img/1466.jpg}}
    \parbox{5cm}{
        \vskip 0.05in
        Q1466: what is on the night stand ?\\
        Ground truth: paper\\
        Object is too small to focus}
&
    \scalebox{0.3}{
        \includegraphics[width=\textwidth, height=.7\textwidth]{daquar_img/2010.jpg}}
    \parbox{5cm}{
        \vskip 0.05in
        Q2010: what is to the right of the table ?\\
        Ground truth: sink\\
        Too many cluttered objects}
\\
\end{array}$
\caption{Variety of Difficulty Levels in DAQUAR}
\label{fig:daquar_easy_hard}
\end{figure*}

\section{Proposed Methodology}
The methodology presented here is two-folded. On the model side we applied recurrent neural networks and visual-semantic embeddings on this task, and on the dataset side we proposed new ways of synthesizing QA pairs from currently available image description dataset.

\subsection{Models}
In recent years, recurrent neural networks (RNNs) had successful applications in the field of natural language processing (NLP). Long short-term memory (LSTM) is a type of RNN which is easier to train than other regular RNNs because of its linear error propagation and multiplicative gatings. There has been increasing interests in using LSTM as encoders and decoders on sentence level. Our model builds directly on top of the LSTM sentence model and is called the ``CNN-LSTM'' model. It treats the image as one word of the question. We borrowed this idea of treating the image as a word from caption generation work done by \cite{vinyals14}. The difference with caption generation is that here we only output the answer at the last time step.

\begin{enumerate}
    \item We used the last hidden layer of the Oxford Conv Net \cite{simonyan14} as our visual embeddings. The CNN part of our model is kept frozen during training.
    \item We experimented with several different word embedding models: randomly initialized embedding, dataset-specific skip-gram embedding and general-purpose skip-gram embedding model \cite{mikolov13}. The word embeddings can either be frozen or dynamic.
    \item We then treated the image as if it is the first word of the sentence. Similar to \cite{frome13}, we used a linear or affine transformation to map 4096 dimension image feature vectors to 300 or 500 dimension that matches the dimension of the word embeddings.
    \item We can optionally treat the image as the last word of the question as well through a different transformation matrix.
    \item We can optionally add a backward direction LSTM, which get the same content but in a backward sequential fashion.
    \item The LSTM(s) output(s) to a softmax layer at the last timestep to classify answers.
\end{enumerate}

\begin{figure}
\centering
\scalebox{0.7}{
\input{imgword.tex}}
\caption{CNN-LSTM model}
\label{fig:imgword}
\end{figure}

\subsection{Datasets}
The currently available DAQUAR dataset only has around 1500 images with 7000 images on 37 common object classes, which might be not enough for training large complex models. Another problem with the current dataset is that the guessing the mode can get very good accuracy. So by creating another dataset, we can make much larger number of QA pairs and more even answer distribution. While collecting human generated QA pairs could be a possible way, we propose to automatically convert descriptions into QA forms. As a starting point we used Microsoft-COCO dataset \cite{mscoco} but it can extend to any other image description dataset such as Flickr dataset \cite{flickr8k}, SBU dataset \cite{ordonez11}, or even directly from the internet. Another advantage of using image description is that they are generated by humans in the first place. So many objects mentioned in the description are easier to notice than the ones in original QAs and synthetic QAs generated from ground truth labellings. It allows the model to rely more on common sense and rough image understanding without any logical reasoning. Lastly the conversion process preserves the language variability in the original description and will result in more human-like questions than questions on image labellings.

Question generation is still an open-ended topic. We are not trying to solve a linguistic problem but just to create a usable image question answering dataset. We prefer undergenerating questions rather than overgenerating. The reason is that currently available image description datasets are large in their sizes, so the quality of questions us more important.

\subsubsection{Common Strategies}
\begin{enumerate}
\item Compound sentences to simple sentences \\
Here we only consider a simple case that is when two sentences are joined together with a conjunctive words. We split the orginial sentences into two independent sentences. For example, ``There is a cat and the cat is running.'' will be split as ``There is a cat.'' and ``The cat is running.''

\item Indefinite determiners to definite determiners.\\
Asking questions on a specific instance of the subject requires changing the determiner into definite form ``the''. For example, ``\textbf{A} boy is playing \underline{baseball}.'' will have ``the'' instead of ``a'' in its question form: ``\underline{What} is \textbf{the} boy playing?''.

\item Wh-movement constraints \\
For English language, questions tend to start with interrogative words such as ``what''. The algorithm needs to move the verb as well as the ``wh-'' constituent to the front of the sentence. In this work we consider the following two simple constraints:
\begin{enumerate}
\item A-over-A principle\\
The A-over-A principle restricts the movement of a wh-word inside an NP \cite{chomsky73}. For example, ``I am talking to John and \textbf{Bill}'' cannot be transformed into ``*\textbf{Who} am I talking to John and'' because ``Bill'' is an noun phrase (NP) that is under another NP ``John and Bill''.
\item Clauses\\
Our algorithm does not move any wh-word that is contained in a clause constituent. This rule can be further fine tuned in the future.
\end{enumerate}
\end{enumerate}

\subsubsection{Pre-Processing}
We used Stanford parser \cite{klein03} to obtain syntatic structure of the original image description.

\subsubsection{Object-Type Questions}
First, we consider asking an object using ``what''. This involves replacing the actual object with a ``what'' in the sentence, and then transforming the sentence structure so that the ``what'' appears in the front of the sentence. The entire algorithm has the following stages:
\begin{enumerate}
\item Split long sentences into simple sentences
\item Change indefinite determiners to definite determiners.
\item Traverse the sentence and identify potential answers and replace with ``what''. During the traversal of object-type question generation, we ignore all the prepositional phrase (PP) consituents because nouns inside prepositions like ``of'' and ``with'' are rarely meaningful answers.
\item Perform wh-movement
\end{enumerate}

Figure~\ref{fig:what_gen} illustrates these procedures with tree diagrams. In order to identify a possible answer word, we used WordNet \cite{wordnet} and NLTK software package \cite{nltk} to get noun categories.

\begin{figure}
    \centering
    \small
    \scalebox{0.8}{
    \Tree [.S [.NP [.DET a ] [.N man ] ] [.VP [.V is ] [.NP [.VBG riding ] [.NP [.DET a ] [.N horse ] ] ] ] ]}
    \scalebox{0.8}{
    \Tree [.S [.WHNP [.WP \textbf{what} ] ] [.S [.V is ] [.NP [.DET \textbf{the} ] [.N man ] ] [.VP [.V \sout{is} ] [.NP [.VBG riding ] [.WHNP [.WP \sout{what} ] ] ] ] ] ]}
    \vspace{5mm}
    \caption{Example: ``A man is riding a \textbf{horse}'' $=>$ ``\textbf{What} is \textbf{the} man riding?''}
    \label{fig:what_gen}
\end{figure}

\subsubsection{Number-Type Questions}
To generate ``how many'' types of questions, we follow a similar procedure as the previous algorithm, except for a different way to identify potential answers. This time, we need to extract numbers from original sentences. Splitting compound sentences, changing determiners, and wh-movement parts remain the same.

\subsubsection{Color-Type Questions}
Color-type questions are much easier to generate. It only requires locating the colour adjective and the noun which the adjective attaches to. Then it simply forms a sentence ``What is the colour of the object'' with the ``object'' replaced by the actual noun.

\subsubsection{Location-Type Questions}
Similar to generating object-type question, except that now the answer traversal will only search within PP contituents with preposition ``in''. We also added rules to filter out clothings so that the answers will mostly be locations, scenes, or large objects that contain smaller objects.

\subsubsection{Post-Processing}
We will show in our experiment results that mode-guessing actually works unexpectedly well in DAQUAR dataset. One of our design requirement of the new dataset is to avoid too common answers.  To achieve this goal, we applied a heuristic to reject the answers that appear too often in generated dataset. First, all QA pairs are shuffled to mitigate dependence between neighboring pairs. We formulate the rejection process as a Bernoulli random process. The probability of enrolling next QA pair $(q, a)$ is:
\begin{equation}
p(q, a) = \left\{ \begin{array}{cl}
1 &\mbox{ if count$(a)$ $\le U$ } \\
\exp\left(-\frac{count(a) - U}{2U}\right) &\mbox{ otherwise }
\end{array} \right.
\end{equation}
where $count(a)$ denotes the current number of enrolled QA pairs that have $a$ as the ground truth answer. After this QA selection process we obtain more uniform distribution of all possible answers.

\section{Experimental Results}

\subsection{Dataset}
Table~\ref{tab:dataset_stats} summarizes the difference between DAQUAR and COCO-QA. COCO-QA has much more data to support the training of more complex models on a larger number of possible answer classes. Another important note is that since we applied QA pair selection process, mode-guessing performs very poorly on COCO-QA; however, since all the answers can be found in the original image descriptions, COCO-QA questions are actually easier to answer from a human point of view. This encourages the model to exploit salient object relations instead of exhaustively search from all possible relations.

\begin{table}[h]
\label{tab:dataset_stats}
\caption{General statistics of DAQUAR and COCO-QA}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{c c c c c}
\hline
\abovespace\belowspace
Dataset        & DAQUAR-37 & COCO-QA \\
\hline
\abovespace
Images         & 795+654   & 80K+40K \\
Questions      & 3825+3284 & 83K+39K \\
Answers        & 63        & 410     \\
\belowspace
GUESS          & 0.1885    & 0.0665  \\
\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\end{table}

Table~\ref{tab:dataset_category_stats} gives a break-down view of all four types of questions present in the COCO-QA dataset with train/test split information.

\begin{table}
\label{tab:dataset_category_stats}
\caption{COCO-QA Question Type Break-Down}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{c c c c c}
\hline
\abovespace\belowspace
Category & Train & \%       & Test  & \%       \\
\hline
\abovespace
Object   & 58352 & 69.96\%  & 27497 & 71.13\%  \\
Number   & 6551  & 7.85\%   & 2575  & 6.66\%   \\
Color    & 13337 & 15.99\%  & 6047  & 15.64\%  \\
\belowspace
Location & 5164  & 6.19\%   & 2541  & 6.57\%   \\
\hline
\belowspace
\abovespace
Total    & 83404 & 100.00\% & 38660 & 100.00\% \\
\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\end{table}

\subsection{Proposed Model Details}
We trained two versions of the architectures that are found to work the best. The first model is CNN and LSTM with a weight matrix in the middle and we call it ``CNN-LSTM model'' in our tables and figures. The second model has two image feature input at the start and the end of the sentence with different linear transformations, and additionally it has two LSTMs going from the forward and backward direction. Both LSTMs output to the softmax layer at the last timestep. We call the second model ``2-CNN-LSTM model'' in our tables and figures.

\subsection{Baselines}
To evaluate the effectiveness of our models, we designed a few baselines. 

\subsubsection{GUESS model}
One very simple baseline is to predict the mode based on question type. For example, if the question contains ``how many'' then the model will output ``two.'' This baseline actually works unexpectedly well in DAQUAR dataset.

\subsubsection{BLIND-BOW model}
To avoid over-interpretation of the results, we designed blind-model baselines which are given only the questions without the images. It is interesting to see how image features are useful in answering the questions. One of the simplest blind model is to simply sum all the word vectors of the question into a bag-of-word (BOW) vector. We then send the sentence features into a softmax layer. Optionally we added a tanh hidden layer before the softmax layer to allow more interaction between image features and word features.

\subsubsection{BLIND-LSTM model}
Another blind model we experimented is to input the question words into the LSTM alone. We could compare it with BLIND-BOW to see how does LSTM contribute to a better language understanding.

\subsubsection{CNN-BOW model}
Similar to BLIND-BOW model but replaced the feature layer with bag-of-word sentence features concatenated with image features from Oxford Net last hidden layer (4096 dimension) after a linear transformation (to 300 or 500 dimension).

\subsection{Performance Metrics}
To evaluate the model, we used the plain answer accuracy as well as the Wu-Palmer similarity (WUPS) measure \cite{wu94, malinowski14b}. The WUPS calculates the similarity between two words based on their longest common subsequence in the taxonomy tree. The similarity function takes in a threshold parameter. If the similarity between two words is less than the threshold then zero score will be given to the candidate answer. It reduces to plain accuracy when the threshold equals to 1.0. Following \cite{malinowski14b}, we measure all the models in terms of plain accuracy and WUPS at 0.9 threshold.

Table~\ref{tab:daquar_results} summarizes the learning results on DAQUAR dataset. Here we compare our results with \cite{malinowski14b}'s model. It should be noted that their results is for the entire dataset whereas our result is on 98.3\% of the original dataset with single word answers.

\begin{table}[h]
\caption{DAQUAR Results}
\label{tab:daquar_results}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{minipage*}{6cm}
\begin{tabular}{c c c c}
\hline
\abovespace
\belowspace
            & Acc.     & WUPS 0.9 \\
\hline
\abovespace
2-Cnn-Lstm  &\#{0.3578}&\#{0.3602}\\
Cnn-Lstm    & 0.3441   & 0.3464   \\
Blind-Lstm  & 0.3273   & 0.3294   \\
Blind-Bow   & 0.3267   & 0.3289   \\
Guess       & 0.1824   & 0.1671   \\
\belowspace
Multi-World & 0.1273   & 0.1810   \\
\hline
\abovespace
\belowspace
Human       & 0.6027   &  0.6104  \\
\hline
\end{tabular}
\end{minipage*}
\end{sc}
\end{small}
\end{center}
\end{table}

\begin{table}[h]
\caption{COCO-QA Results}
\label{tab:cocoqa_results}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{c c c c}
\hline
\abovespace\belowspace
           & Acc.     & WUPS 0.9 \\
\hline
\abovespace
2-Cnn-Lstm &\#{0.5161}&\#{0.5244}\\
Cnn-Lstm   & 0.5073   & 0.5153   \\
Cnn-Bow    & 0.4490   & 0.4593   \\
Blind-Lstm & 0.3516   & 0.3592   \\
Blind-Bow  & 0.3262   & 0.3337   \\
\belowspace
Guess      & 0.0665   & 0.0851   \\
\hline
\abovespace
\end{tabular}
\end{sc}
\end{small}
\end{center}
\end{table}

\begin{table}[h]
\caption{COCO-QA Accuracy Per Category Break-Down}
\label{tab:cocoqa_acc_breakdown}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{c c c c c}
\hline
\abovespace\belowspace
           & Object    & Number   & Color    & Location \\
\hline
\abovespace
2-Cnn-Lstm & \#{0.5386}& 0.4534   &\#{0.4786}& 0.4258   \\
Cnn-Lstm   & 0.5321    &\#{0.4678}& 0.4439   &\#{0.4294}\\
Cnn-Bow    & 0.4718    & 0.3773   & 0.4130   & 0.3609   \\
Blind-Lstm & 0.3459    & 0.4383   & 0.3334   & 0.3680   \\
Blind-Bow  & 0.3201    & 0.3339   & 0.3434   & 0.3436   \\
\belowspace
Guess      & 0.0211    & 0.3584   & 0.1387   & 0.0893   \\
\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\end{table}


\section{Discussion}
From the above results we demonstrated our models is doing a reasonable job in the image question answering classification problem. It outperforms the baselines and the existing approach in terms of answer accuracy. It is amazing to see that the blind model is in fact very strong on DAQUAR dataset, even comparable to CNN-LSTM models. We speculate that it is likely that the ImageNet image examples is very different from the indoor scene images which are mostly furnitures. So therefore the CNN-LSTM cannot really make use of the image features unless the question is asking about the largest object, i.e. differentiating between sofas, beds, and tables. However, the CNN-LSTM model outperforms the blind model by a large margin in COCO-QA dataset. There are three possible reasons behind. First, the objects in MS-COCO dataset resemble the ones in ImageNet more; second, the images have less number of objects whereas the indoor scenes have many cluttered objects in one image; and third, COCO-QA has more data to train complex models. 

There are many interesting examples but due to the space limit we can only show a few in Figure~\ref{fig:object_questions}, \ref{fig:number_questions}, \ref{fig:color_questions}. The full result is available to view at (url will be ready upon the release of the paper). For some of the examples, we specifically tested extra questions (the ones have ``a'' in the question ID) to avoid over-interpretating the questions that CNN-LSTM accidentally got correct. The parentheses in the figures represent the confidence score given by the softmax layer of the models. ``DQ'' denotes questions from DAQUAR dataset and ``CQ'' denote questions from COCO-QA dataset.

\subsection{Model Selection}
We did not find that using different word embedding has a significant impact on the final classification results. The best model for DAQUAR is using a randomly initialized embedding of 300 dimensions whereas the best model trained for COCO-QA is using a problem-specific skip-gram embedding initialize. We find that normalizing the CNN hidden layer image features before send to linear/affine transformation helps the training become smoother because the image features will blend with the word embedding space with similar statistics. The bidirectional LSTM model can further boost the result by a little.

\subsection{Object}
As the original CNN was trained for the ImageNet \cite{ilsvrc14} challenge, the CNN-LSTM benefited largely from the object recognition ability. For some simple object pictures in COCO-QA dataset, the CNN-LSTM and CNN-BOW can easily get the correct answer just from the image features. However, the challenging part is to consider spatial relations between multiple objects and probably focus on details of the image. Some qualitative results in Figure~\ref{fig:object_questions} shows that the CNN-LSTM only does a moderately acceptable job on it. Sometimes it fails to make correct decision but keep output the most salient object or sometimes the blind model can equally guess the most probable objects based on the question alone (e.g. chairs should be around the dinning table).

\subsection{Counting}
In DAQUAR, we cannot observe any advantage in counting of the CNN-LSTM model compared to other baselines. In COCO-QA there is some observable counting ability in very clean image with single object type. The model can sometimes count up to five or six. However, as shown in Figure~\ref{fig:number_questions}, the ability is fairly weak as it does not count correctly when different object types are present. As the CNN-LSTM model only wins the blind one by 3\% on counting, there will be very large space for improvement in the counting task and in fact it could be a separate computer vision problem by its own.

\subsection{Color}
In COCO-QA there is significant difference between CNN-LSTM model and BLIND-LSTM model on color-type questions. We further discovered that the model not only can recognize the dominant color of the image but can sometimes associating different color to different object, as shown in Figure~\ref{fig:color_questions}. However, the model is still not very robust and fails on a number of easy examples.

\subsection{Limitations and Future Work}
Image question answering is a fairly new research topic, and the approaches we present here has a number of limitations. First the model is just an answer classifier. Ideally we would like to permit longer answers which will involve some sophisticated text generation model or structured output learning. Secondly, our question generation algorithm also assumes that all answers are single word, and the implementation of the algorithm is heavily dependent on the question type. Also, the algorithm is only applicable to English language at this time. In this paper, the question generation was only serving for our models so the above limitations were not big concerns. Lastly, our approach is hard to interpret why the model output a certain answer. By comparing the model to some baselines we can roughly infer but humans are still prone to over-interpretion of the results. Visual attentions might be future direction for us which could also help explaining the model output.

\begin{figure*}[hb!]
\centering\small
$\begin{array}{p{5cm} p{5cm} p{5cm}}

    \scalebox{0.3}{
        \includegraphics[width=\textwidth, height=.7\textwidth]{cocoqa_img/5429.jpg}}
    \parbox{5cm}{
        \vskip 0.05in
        CQ5429: what do two women hold with a picture on it ?\\
        Ground truth: cake\\
        2-CNN-LSTM: \textcolor{green}{cake (0.5611) }\\
        CNN-BOW: \textcolor{red}{laptop (0.1443) }\\
        BLIND-LSTM: \textcolor{red}{umbrellas (0.1567) }\\
        BLIND-BOW: \textcolor{red}{phones (0.1447) }
}
&
    \scalebox{0.3}{
        \includegraphics[width=\textwidth, height=.7\textwidth]{cocoqa_img/24952.jpg}}
    \parbox{5cm}{
        \vskip 0.05in
        CQ24952: what is the black and white cat wearing ?\\
        Ground truth: hat\\
        2-CNN-LSTM: \textcolor{green}{hat (0.6349) }\\
        BLIND-LSTM: \textcolor{red}{tie (0.5821) }\\
}
&
    \scalebox{0.3}{
        \includegraphics[width=\textwidth, height=.7\textwidth]{cocoqa_img/25218.jpg}}
    \parbox{5cm}{
        \vskip 0.05in
        CQ25218: where are the ripe bananas sitting ?\\
        Ground truth: basket\\
        2-CNN-LSTM: \textcolor{green}{basket (0.4965) }\\
        BLIND-LSTM: \textcolor{red}{bowl (0.6415) }\\
        CQ25218a: what are in the basket ?\\
        Ground truth: bananas\\
        2-CNN-LSTM: \textcolor{green}{bananas (0.6443) }\\
        BLIND-LSTM: \textcolor{red}{bears (0.0956) }
}
\\
    \scalebox{0.3}{
        \includegraphics[width=\textwidth, height=.7\textwidth]{daquar_img/585.jpg}}
    \parbox{5cm}{
        \vskip 0.05in
        DQ585: what is the object on the chair?\\
        Ground truth: pillow\\
        2-CNN-LSTM: \textcolor{green}{pillow (0.6475) }\\
        BLIND-LSTM: \textcolor{red}{clothes (0.3973) }\\
        DQ585a: where is the pillow found?\\
        Ground truth: chair\\
        2-CNN-LSTM: \textcolor{green}{chair (0.1735) }\\
        BLIND-LSTM: \textcolor{red}{cabinet (0.7913) }
}
&
    \scalebox{0.3}{
        \includegraphics[width=\textwidth, height=.7\textwidth]{daquar_img/2136.jpg}}
    \parbox{5cm}{
        \vskip 0.05in
        DQ2136: what is right of table?\\
        Ground truth: shelves\\
        2-CNN-LSTM: \textcolor{green}{shelves (0.2780)}\\
        BLIND-LSTM: \textcolor{green}{shelves (0.2000)}\\
        DQ2136a: what is in front of table?\\
        Ground truth: chair\\
        2-CNN-LSTM: \textcolor{green}{chair (0.3104)}\\
        BLIND-LSTM: \textcolor{green}{chair (0.3661)}\\
        Sometimes the blind model can infer indoor object relations without looking at the image.
}
&
    \scalebox{0.3}{
        \includegraphics[width=\textwidth, height=.7\textwidth]{cocoqa_img/2007.jpg}}
    \parbox{5cm}{
        \vskip 0.05in
        CQ2007: what is next to the cup?\\
        Ground truth: banana\\
        2-CNN-LSTM: \textcolor{green}{banana (0.3560) }\\
        BLIND-LSTM: \textcolor{red}{sandwich (0.0647) }\\
        CQ2007a: what is sitting next to a banana?\\
        Ground truth: cup\\
        2-CNN-LSTM: \textcolor{red}{banana (0.4539) }\\
        BLIND-LSTM: \textcolor{red}{cat (0.0538) }\\
        In this example the CNN-LSTM model fails to output different objects based on spatial relations.
}
\end{array}$
\caption{Qualitative Evaluation on Object-Type Questions}
\label{fig:object_questions}
\end{figure*}

\begin{figure*}[ht!]
\centering\small
$\begin{array}{p{5cm} p{5cm} p{5cm}}

    \scalebox{0.3}{
        \includegraphics[width=\textwidth, height=.7\textwidth]{cocoqa_img/6805.jpg}}
    \parbox{5cm}{
        \vskip 0.05in
        CQ6805: how many uncooked doughnuts sit on the baking tray?\\
        Ground truth: six\\
        2-CNN-LSTM: \textcolor{green}{six (0.2779) }\\
        CNN-BOW: \textcolor{red}{twelve (0.2342) }\\
        BLIND-LSTM: \textcolor{red}{four (0.2345) }
}
&
    \scalebox{0.3}{
        \includegraphics[width=\textwidth, height=.7\textwidth]{cocoqa_img/32912.jpg}}
    \parbox{5cm}{
        \vskip 0.05in
        CQ32912: how many bottles of beer are there?\\
        Ground truth: three\\
        2-CNN-LSTM: \textcolor{green}{three (0.4153) }\\
        CNN-BOW: \textcolor{red}{two (0.4849)}\\
        CQ32912a: how many bananas are there?\\
        Ground truth: two\\
        2-CNN-LSTM: \textcolor{red}{three (0.1935)}\\
        CNN-BOW: \textcolor{green}{two (0.5307)}
}
&
    \scalebox{0.3}{
        \includegraphics[width=\textwidth, height=.7\textwidth]{daquar_img/1520.jpg}}
    \parbox{5cm}{
        \vskip 0.05in
        DQ1520: how many shelves are there ?\\
        Ground truth: three\\
        2-CNN-LSTM: \textcolor{red}{two (0.4801) }\\
        BLIND-LSTM: \textcolor{red}{two (0.2060) }\\
        DQ1520a: how many sofas are there ?\\
        Ground truth: two\\
        2-CNN-LSTM: \textcolor{green}{two (0.6173) }\\
        BLIND-LSTM: \textcolor{green}{two (0.5378) }\\
        In the last two examples the model does not know how to count with different object types.
}
\end{array}$
\caption{Qualitative Evaluation on Number-Type Questions}
\label{fig:number_questions}
\end{figure*}

\begin{figure*}[hb!]
\centering\small
$\begin{array}{p{5cm} p{5cm} p{5cm}}
    \scalebox{0.3}{
        \includegraphics[width=\textwidth, height=.7\textwidth]{daquar_img/2989.jpg}}
    \parbox{5cm}{
        \vskip 0.05in
        DQ2989: what is the colour of the sofa?\\
        Ground truth: red\\
        2-CNN-LSTM: \textcolor{green}{red (0.2152) }\\
        BLIND-LSTM: \textcolor{red}{brown (0.2061) }\\
        DQ2989a: what is the colour of the table?\\
        Ground truth: white\\
        2-CNN-LSTM: \textcolor{green}{white (0.4057) }\\
        BLIND-LSTM: \textcolor{green}{white (0.2751) }
}
&
    \scalebox{0.3}{
        \includegraphics[width=\textwidth, height=.7\textwidth]{cocoqa_img/6200.jpg}}
    \parbox{5cm}{
        \vskip 0.05in
        CQ6200: what is the color of the cone?\\
        Ground truth: yellow\\
        2-CNN-LSTM: \textcolor{green}{yellow (0.4250) }\\
        CNN-BOW: \textcolor{red}{white (0.5507) }\\
        BLIND-LSTM: \textcolor{red}{orange (0.4473) }\\
        CQ6200a: what is the color of the bear ?\\
        Ground truth: white\\
        2-CNN-LSTM: \textcolor{green}{white (0.6000) }\\
        CNN-BOW: \textcolor{green}{white (0.5518) }\\
        BLIND-LSTM: \textcolor{red}{brown (0.5518) }
}
&
    \scalebox{0.3}{
        \includegraphics[width=\textwidth, height=.7\textwidth]{cocoqa_img/6058.jpg}}
    \parbox{5cm}{
        \vskip 0.05in
        CQ6058: what is the color of the coat?\\
        Ground truth: yellow\\
        2-CNN-LSTM: \textcolor{green}{yellow (0.2942) }\\
        BLIND-LSTM: \textcolor{red}{black (0.2456) }\\
        CQ6058a: what is the color of the umbrella?\\
        Ground truth: red\\
        2-CNN-LSTM: \textcolor{red}{yellow (0.4755)}\\
        BLIND-LSTM: \textcolor{red}{purple (0.2243)}
}
\end{array}$
\caption{Qualitative Evaluation on Color-Type Questions}
\label{fig:color_questions}
\end{figure*}

\section{Conclusion}
In this paper, we consider the image QA problem and present our CNN-LSTM model by combining CNN and LSTMs with visual-semantic embeddings. As the currently available dataset is not large enough, we designed an algorithm that helps us collect large scale image QA dataset from image descriptions. Our model shows a reasonable understanding of the question and some coarse image understanding, but it is still very naive in many situations. Moreover, our question generation algorithm is extensible to many image description datasets and can be automated without the involvement of human labor. We hope that the creation of the new dataset can encourage more data-driven approaches to this problem in the future.

% Acknowledgements should only appear in the accepted version. 
\section*{Acknowledgments}

\bibliography{Reference}
\bibliographystyle{icml2015}
\end{document} 