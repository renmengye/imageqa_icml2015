 Methods and Results 
 Problem restatement 
We consider the dataset of image-based question answering as a triplet of an image vector, a question of a sequence of word indices, and an answer of a sequence of word indices. We start off by assuming that the answers are all one-word answers, i.e. the length of the answer sequence is always one. The assumption is established based on more than 98\

 Image-word model 
 LSTM sentence model 
Due to the sequential nature of natural languages, researchers have been looking for ways to model sentences using recurrent neural networks. There has been increasing interests in using LSTM to model the embedding vector for a whole sentence. Recently,  sutskever14  uses LSTM as both encoders and decoders in machine translation, and achieved BLEU score  bleu  better than the traditional phrase-based models  luong14 . 

 Image as first word 
We start off the experiment for image-based question answering by directly building on top of the sentence model. 

In this experiment, we designed a model called the ``image-word model'' because it treats the image as if being one word of the question. We borrowed the idea of treating image as the first word of the image from  vinyals14  in their caption generation paper. The difference with the caption generation is that here we are only output the answer at the last time step.

	 We use the last hidden layer of the Oxford convolutional neural net (4096 dimension)  simonyan14  as our visual embedding model.
	 We use the word2vec embedding model (skip gram) trained from Google News  mikolov13  as our frozen semantic embedding (300 dimension). 
	 We then treat the image as if it is the first word of the sentence. In order to map the image embedding to our latent semantic embedding space, we used a linear transformation layer of dimension 4096\times300 (similar to  frome13 ).
	 The last time step of the LSTM output is passed to a softmax layer. The final output is a 68-dimension vector representing the probability of the answering being each possible answer classes.

To simplify our model, we used the 37 class dataset, so the question and answers will only focus on 37 object classes. We also filtered dataset down to only one-word answers. This simplification trims the possible set of answers to 63 words (including unknown class ``UNK'').
To evaluate the model, we used the plain answer accuracy as well as the Wu-Palmer similarity (WUPS) measure  wu94, malinowski14b . The WUPS calculuates the similarity between two words based on their longest common subsequence in the taxonomy tree. The similarity function takes in another threshold parameter. If the similarity between two words is less than the threshold then zero score will be given to the candidate answer. It reduces to plain accuracy when threshold equals to 1.0. Following  malinowski14b , we measure all the models in terms of plain accuracy, WUPS at 0.9 threshold, and WUPS at 0.0 threshold.

To compare the results, we also designed a baseline model to avoid over-interpretation. In this model, the images are replaced by randomly generated vectors (noises) instead of meaningful graphics. We call this baseline the ``blind model.'' The training detail of the models above can be found in Appendix  sec:imgword . Table  tab:imgword  summarizes the performance of  image-word model compared to our baseline and the model in DAQUAR.


 0.9  
 imgword.tex  
 Image-word model and blind model 
 fig:imgword 

First, our model wins by a large margin compared to the results from the publisher of the DAQUAR dataset. Second, we argue that most gain in accuracy results from a good sentence embedding, because the image-word model has almost the same accuracy compared to the blind model. Third, the higher similarity score obtained by the image-word model suggests that the visual embedding still has some positive influence in making correct decisions. 

In Figure  fig:imgword+blind , we further discovered the some weak visual ability by a direct comparison of test examples. We observed that image-word model seems to perform better on questions centered on the dominant objects or the dominant colors, but does not seem to learn how to count.

    
    
     subfigure [t] 0.3 
            [width=\textwidth] bed_vs_table.jpg 
            Q193: what is the largest object ?
            Correct answer: table
            Image-word: table (0.576)
            Blind: bed (0.440)
             The question gives no extra clue of the class of the object. The visual model recognizes the correct object through the shape. 
     subfigure 
    
     subfigure [t] 0.3 
            [width=\textwidth] fridge_vs_toilet.jpg 
            Q212: what is the object left of the room divider ?
            Correct answer: door
            Image-word: toilet (0.3973), sink (0.1367), towel (0.1323)
            Blind: refridgerator (0.5318)
             The visual model recognizes the correct scene. 
     subfigure 
    
     subfigure [t] 0.3 
            [width=\textwidth] three_vs_three.jpg 
            Q1615: how many pictures are there on the wall ?
            Correct answer: seven
            Image-word: three (0.4518)
            Blind: three (0.6047)
             No evidence shows that image-word model learns how to count. 
     subfigure 
     Direct comparison between image-word model and blind model. 
     fig:imgword+blind 

 Bidirectional image-word model 
The image-word model takes in word sequentially in a linear fasion may not be powerful enough to handle complex sentence structures in the question. At the last timestep it needs to output the answer immediately after reading the last word of the question. In some cases where the hint is revealed at the last timestep, the single direction model maybe failed to change direction because the previous words has made the model very confident of an answer already. The bidirectional image-word model aims to fix this weakness. It contains two LSTMs. The first one takes the image as the first word and then consumes the words sequentially just like the previous model. Moreover, the second LSTM consumes the words in an reverse order, together with the hidden state of the first LSTM at that timestep. At the last timestep, the second LSTM takes in the image again. So intuitively, the first timestep gives an ``impression'' of the image, and after knowing the sentence, it checks the image again to confirm the answer. It is worth to note that the second LSTM ``sees'' the whole sentence at every timestep.


 0.9  
 bidir_imgword.tex  
 Bidirectional image-word model and blind model 
 fig:imgword 

 Cross entropy vs. ranking loss 










This type of model outputs a vector that is the nearest neighbor of the answer word in the semantic embedding. The error function uses the pair-wise ranking loss  weston10 , defined below:

\limits_ \+y  \sum\limits_ i \neq j  \max  0, \alpha-s(\+y, \+a_j) + s(\+y, \+a_i)  

\+y and \+a are both vectors in the answer embedding space. \+y is the output of the model, \+a_j is the correct answer for the question, and \+a_i is one of any possible answers. s(, \cdot) denotes the similarity measure of two vectors. It is usually implemented as the cosine similarity in high dimensional spaces: 
s(\+x, \+y) =  \+x  \+y    \+x   \+y  

This error function will penalize the model if the right answer is not winning over other wrong answers by a certain margin . It is a widely used in ranking image descriptions  kiros14b . 

The ranking loss effectively replaces the softmax layer, and the hidden dimension of the LSTM need to match with the word embedding dimension. In our experiments, the ranking loss models do not outperform the softmax models. Although the training is much faster even with smaller learning rates, it achieves lower accuracy because it tends to overfit the training set too quickly. Figure [cite] compares the training curve of both the ranking models and the softmax models on DAQUAR dataset.


 DAQUAR results 


                 &  Accuracy  &  WUPS 0.9  &  WUPS 0.0 

 2-IMGWD  Bidirectional image-word model  &  0.3276  &  0.3298  & 0.7272
 IMGWD  Image-word model    & 0.3188   & 0.3211 &  0.7279 
 IMGWD-RK  Image-word model with ranking loss    & 0.2787 & 0.2782 & 0.7074

 BLIND  Single-direction blind model    & 0.3051   & 0.3069 & 0.7229
 RANDWD  Single-direction model with random word embedding   & 0.3036   & 0.3056 & 0.7206
 BOW  Bag-of-words model      &  0.2299  & 0.2340 & 0.6907
 GUESS  Guess ``two'', ``white'', and ``table'', depending on question type    & 0.1785   & 0.1823       & 0.6874
 MultiWorld  malinowski14b   & 0.1273 & 0.1810 & 0.5147

 HUMAN  & 0.6027 & 0.6104 & 0.7896


 COCO-QA dataset 
From results on the DAQUAR dataset, we see that although our model has improved the previous attempt by a lot on the answer accuracy, the blind version of the model can do almost equally well, suggesting that the image features from the CNN is not very useful. Since 1500 images are a very small dataset, maybe it is worthwhile for us to invest time building a larger dataset so that the neural networks can be trained more robustly.

Manually annotating pictures with multiple QA pairs requires large amount of time, capital, and tools. We propose to use currently massively available image description dataset and convert description into QA forms. This method is cheap, fast, and scalable; however the drawback is that the quality of automatically generated questions is unforeseeable. There lacks a measure to test how sensical the questions are overall.

We used recently released Microsoft COCO [cite] dataset, which contains 100K images and 500K sentence descriptions, and converted the descriptions into QA pairs. We only consider three types of questions: object, number, and color, and we only generate single-word answers.


 Question conversion algorithms 
Here we present an algorithm that converts sentences into different types of questions.

 Object-type questions 
First, we consider asking an object using ``what''. This involves replacing the actual object with a ``what'' in the sentence, and then transforming the sentence structure so that ``what'' appears in the front of the sentence. The algorithm below assumes that the input is a sentence in the form of a syntactic tree, and outputs a question in the form of a syntactic tree.

The entire algorithm has the following stages:
 Split long sentences into simple sentences. 
For question generation task, sentences need not to be very descriptive, and shorter the original sentences are, more readable the questions will be. Moreover, the wh-words that are found in sub-sentences or clauses will less likely be able to perform wh-movement [cite]. Here we only consider a simple case that is when two sentences are joined together with a conjunctive word ``and''. We split the orginial sentences into two independent sentences.

 Traverse the sentence and identify potential answers. 
We traverse the tree structure and identify answers that belong to certain word lexical category. Replace the answer with a question word ``what'' or ``who'' depending on the object type.

 Change indefinite determiners to definite determiners
Asking questions is referring to a specific instance of the subject. Therefore changing the determiner into definite form ``the'' will make the question more grammatical. For example, `` A  boy is playing  baseball .'' goes to `` What  is  the  boy playing?''.

 Perform wh-movement with some constraints. 
For English language, questions tend to start with interrogative words such as ``what'' and ``who''. To implement this feature, the algorithm needs to move the verb as well as the ``wh-'' constituent to the front of the sentence. However, not all sentences allow such transformation. In this work we consider the following constraints:
 A-over-A principle
This was first discovered by [cite]. For example, ``I am talking to John and  Bill '' cannot be transfromed into ``*Who am I talking to John and'' because ``Bill'' is an noun phrase (NP) constituent that is under another NP ``John and Bill''. Therefore, the child NP cannot be removed from the parent NP in the wh-movement.
 Clauses
Except for a few cases, interrogative words in clauses usually cannot be moved to the front of the sentences. For example, ``I am riding a motorcycle that  Bill  wanted.'' cannot be transformed into ``*Who am I riding a motorcycle that wanted.''. In the future, there is a need to separate the original sentences into two: ``I am riding a motorcycle.'' and ``Bill wanted the motorcycle.''. For now, wh-word in the clauses will terminate the wh-movement process and our algorithm will only output sentences like ``I am riding a motorcycle that  who  wanted?''.

Figure[cite] illustrates these procedures with an example.

The details of the algorithms are presented below. We used wordnet [cite] and NLTK software package [cite] to lemmatize verbs and to get lex names of nouns. We also used Stanford parser [cite] to get syntatic structure of original description.
 Split composite sentences 
 Root of the syntactic tree of the original sentence
 List of roots of syntactic trees of split sentences
 SplitCC  root 
 node \gets root.children[0]  Search directly from ``S'' node 
 node is ``S'' and it has more than 3 children 
     All children are ``S'' or ``CC'' and ``CC''s are always in between two ``S''s 
         \Return each ``S'' child
    



 Replace indefinite determiner to definite determiner on subjects 
 Root of the syntactic tree with wh-word in the original place
 Root of the syntactive tree with wh-word in the front
 SwitchDefDet  root 
 node \gets  DFS  root, ``NP''   Search for the subject in the sentence 
 node \gets  DFS  node, ``DT''   Search for the determiner of the subject 
 node.text = ``a''  or  node.text = ``an'' 
     node.text \gets ``the''

 \Return root




    
    
     subfigure [t] 0.45 
         Step 1: Parse the sytactic structure. 
        
         0.8  
         [.S [.NP [.DET A ] [.N man ] ] [.VP [.V is ] [.NP [.VBG riding ] [.NP [.DET a ] [.N horse ] ] ] ] ] 
         5mm 
     subfigure 
    
     subfigure [t] 0.45 
         Step 2: Change determiner. 
        
         0.8  
         [.S [.NP [.DET  the  ] [.N man ] ] [.VP [.V is ] [.NP [.VBG riding ] [.NP [.DET a ] [.N horse ] ] ] ] ] 
        
         5mm 
     subfigure 
     subfigure [t] 0.45 
         Step 3: Find the answer of the question. 
        
         0.8  
         [.S [.NP [.DET the ] [.N man ] ] [.VP [.V is ] [.NP [.VBG riding ] [.NP [.DET  a  ] [.N  horse  ] ] ] ] ] 
         5mm 
     subfigure 
    
     subfigure [t] 0.45 
         Step 4: Replace the answer with ``what''. 
        
         0.8  
         [.S [.NP [.DET the ] [.N man ] ] [.VP [.V is ] [.NP [.VBG riding ] [.NP [.DET  a  ] [.N  horse  ] ] ] ] ] 
         5mm 
     subfigure 
     subfigure [t] 0.45 
         Step 4: Replace the answer with ``what''. 
        
         0.8  
         [.S [.NP [.DET A ] [.N man ] ] [.VP [.V is ] [.NP [.VBG riding ] [.WHNP [.WP  what  ] ] ] ] ] 
         5mm 
     subfigure 
    
     subfigure [t] 0.45 
         Step 5: Perform WH-fronting. 
        
         0.8  
         [.S [.WHNP [.WP What ] ] [.S [.V is ] [.NP [.DET the ] [.N man ] ] [.VP [.V  is  ] [.NP [.VBG riding ] [.WHNP [.WP  what  ] ] ] ] ] ] 
         5mm 
     subfigure 
     
        Example: ``A man is riding a  horse '' => `` What  is  the  man riding?'' 




 Wh-movement 
 Root of the syntactic tree with wh-word in the original place
 Root of the syntactive tree with wh-word in the front
 Wh-movement  root 

 Check if the sentence contains a verb-phrase. If no, terminate.
 Check if WHNP is under ``SBAR'' or ``NP''. If yes, terminate.

 verbFront \gets  null 
 The verb is any tensed form of ``be'' or ``have done'', or is a modifier e.g. ``will'' 
     verbFront \gets verb
     Remove verb from its parent

     verbFront \gets Node(class=verb.class, text=``does/do/did'', children=[])
     verb \gets  Lemmatize  verb 


 Remove WHNP from its parent
 S_old \gets root.children[0]
 S_old.children.insert(verbFront, 0)  Insert verbFront as the first child of S_old 
 S_new \gets Node(class=``S'', text=``'', children=[WHNP, S_old])
 root.children \gets [S_new]
 \Return root


 Identify object-type answers 
 traverseWhatAlg 
 Root of the syntactic tree of the original sentence
 List of roots of the syntactic trees with answers replaced by ``what''
 TraverseWhat  root, results 
 child  node.children 
      TraverseWhat  child, results 

 node is``NP'' and node's children contains a noun with lexname
        \algorithmicindent  animal, artifact, body, food, object, plant, possession, shape, person 
     answer \gets noun
     whatNode \gets Node(class=``WP'', text=``who/what'', children=[ ])
     Replace answer with Node(class=``WHNP'', text=``'',  children=[whatNode])
     Insert the modified root into roots list




 Generate object-type questions 
 askWhatAlg 
 Root of the syntactic tree of the original sentence
 List of roots of the syntactic trees of generated questions

 AskWhat  Root 
  SwitchDefDet  root 
  r \in  SplitCC  root  
     roots \gets [ ],  TraverseWhat  r, roots 
      r2 \in roots 
          yield return   Wh-movement  r2 
    




 Number-type questions 
To generate ``how many'' types of questions, we follow a similar procedure as previous algorithms, except for a different way to identifying potential answers. This time, we need to extract numbers from the original sentences. Splitting composite sentences, wh-movement, and changing determiner parts remain the same. Algorithm [cite] shows an detailed algorithm of how to generate number-type questions from a sentence.

 Identify number-type answers 
 traverseHowManyAlg 
 Root of the syntactic tree of the original sentence
 List of roots of the syntactic trees with answers replaced by ``how many''
 TraverseHowMany  root, results 
 child  node.children 
      TraverseHowMany  child, results 

 answer \gets  null 
 node is ``NP'' and node's children contains number 
     howNode \gets Node(class=``WRB'', text=``how'', children=[ ])
     manyNode \gets Node(class=``JJ'', text=``many'', children=[ ])
     parent \gets Node(class=``WHNP'', text=``'', children=[howNode, manyNode])
     Replace the number node with parent
     Insert the modified root into results list



 Generate number-type questions 
 askHowManyAlg 
 Root of the syntactic tree of the original sentence
 List of roots of the syntactic trees of generated questions

 AskHowMany  Root 
 results \gets [ ]
  SwitchDefDet  root 
  r \in  SplitCC  root  
     roots \gets [ ],  TraverseHowMany  r, roots 
      r2 \in roots 
          yield return   Wh-movement  r2 
    



 Color-type questions 
Compared to the previous two questions types, color-type questions are much easier to generate. It only requires locating the color adjective and the noun which the adjective attaches to. Then it simply forms a sentence ``What is the color of the object'' with the ``object'' replaced by the actual noun. The tree traversal is also similar to previously presented algorithms so I will omit the implementation details.

 Summary 
Above we show how to generate three types of questions from sentences. There are mainly two sources of errors: first is caused by incorrect sentence structures produced by the Stanford parser and second is caused by our algorithms. It is hard to evaluate the quality of the generated questions due to its large amount. A sample of the first 100 questions shows a precision of (number here) (i.e. \
 Sentence fragments
Many descriptions in Microsoft COCO are not complete sentences. For example ``A bathroom with toilet'' or ``A man riding a bike''. Further work is needed to fill in missing verbs in those incomplete sentences.
 Split clauses
Just like the way we separate composite sentences that have conjunction word ``and'', we can also separate clauses from the original sentence.
 Identify phrasemes as one answer entity
Sometimes multiple words represent a single entity, also called phrasemes. For example, ``fire hydrant'' and ``bathroom sink'' should be regarded as one single word in the answer.
 Wh-movement with more sophisticated verb insertion
As mentioned above, many image descriptions are sentence fragments. We should have a generic way to insert appropriate verbs in the questions.

 Reducing rare answers 
Here we impose a hard constraint such that all answers must at least appear L times in the dataset. In the 6.6K dataset, L=5, and in the full dataset, L=20.

 Reducing common answers 
Besides creating more data, the other main motivation to create a new dataset is to reduce the performance of a model that simply guesses for the three most common answers for each type of question (e.g. ``man'', ``two'', ``white''). Therefore, additional scheme needs to be employed to reduce the commmon answers. The probability of enrolling an question-answer pair (q, a) is:
p(q, a) =    array  cl 
1 &  if  count (a) \le U   
\left(- \text count (a) - U  2U \right) &  otherwise  
where U=100 in both the 6.6K dataset and the full dataset. This will penalize answers for appearing more than U times while still preserve the rank of the most common answers.

 Question statistics 
With the automatic question generation technique described above, we gathered a dataset from the MS-COCO dataset. We name this dataset to be COCO-QA dataset. Table [cite] presents some important statistics of COCO-QA, with comparison to DAQUAR. The full dataset is considered to be harder than DAQUAR, because it has more possible answer classes and lower guess baseline rate. However, COCO-QA contains 60 times more images and 20 times more question-answer pairs, so it is more suitable to deep learning approaches.


 Dataset comparison 

 Dataset  &  \# Images  &  \# Questions  &  \# Answers Number of answers present in the training set plus ``UNK'' class   &  GUESS accuracy  Guess the three most common answers depending on question type 

DAQUAR-37 Trimmed to include only single-word answer   &  795+654 ``+'' denotes train-test split   & 3825+3284  & 63     & 0.1885
DAQUAR-894 Trimmed to include only single-word answer   &  795+654  & 6149+5076  & 412    & 0.1180
COCO-QA 6.6K & 3.6K+3K      & 14K+11.6K  & 298        & 0.1157         
COCO-QA Full & 80K+20K      & 177K+83K & 794           & 0.0347



 Dataset question type break-down 

 Dataset       &         &  Object  &  Number  &  Color  &  Total   

 DAQUAR-37     & Train   & 2814 (0.7357)   & 903 (0.2361)    & 108 (0.0282)   & 3825 (1.0000)   
                      & Test    & 2406 (0.7326)   & 779 (0.2372)    & 99 (0.0301)    & 3284 (1.0000)   

 DAQUAR-894    & Train   & 5131 (0.8344)   & 903 (0.1469)    & 115 (0.0187)   & 6149 (1.0000)   
                      & Test    & 4194(0.8262)    & 779(0.1535)     & 103(0.0203)    & 5076(1.0000)    


 COCO-QA 6.6K  & Train   & 10759 (0.7641)  & 1134 (0.0805)   & 2187 (0.1553)  & 14080 (1.0000)  
                      & Test    & 8850 (0.7645)   & 939 (0.0811)    & 1878 (0.1544)  & 11576 (1.0000)  

 COCO-QA Full  & Train   & 155888 (0.8824) & 7213 (0.0408)   & 13561 (0.0768) & 176662 (1.0000) 
                      & Test    & 73483  (0.8875) & 3181 (0.0384)   & 6135 (0.0741)  & 82809 (1.0000)  



 Learning results 
We applied the same models, the single directional and the bi-directional image-word model, to the COCO-QA dataset. Table [cite] summarizes the results. We further computed the accuracy in each question category in Table [cite]

 COCO-QA results 

                 &          & 6.6K &             &          & Full &           

                 &  Acc.      &  WUPS 0.9    &  WUPS 0.0        &  Acc.      &  WUPS 0.9    &  WUPS 0.0      


 2-IMGWD  &  0.3358  &  0.3454  &  0.7534  &  0.3208  &  0.3304  &  0.7393  
 IMGWD    & 0.3260   & 0.3357     & 0.7513       & 0.3153   & 0.3245     & 0.7359     


 BOW      & 0.1910   & 0.2018     & 0.6968       & 0.2365   & 0.2466     & 0.7058     
 RANDWD   & 0.1366   & 0.1545     & 0.6829       & 0.1971   & 0.2058     & 0.6862     
 BLIND    & 0.1321   & 0.1396     & 0.6676       & 0.2517   & 0.2611     & 0.7127     
 GUESS    & 0.1157   & 0.1608     & 0.6893       & 0.0347   & 0.0609     & 0.6376      




 Full COCO-QA accuracy per category break-down 

                   &  Object  &  Number  &  Color   &  Total   

 2-IMGWD    &  0.3139  &  0.4233  & 0.3600          &  0.3208  
 IMGWD      & 0.3078          & 0.4189          &  0.3604  & 0.3153         

 BOW        & 0.2273          & 0.3398          & 0.3027          & 0.2365          
 BLIND      & 0.2389          & 0.3713          & 0.3539          & 0.2517          
 RANDWD     & 0.1828          & 0.3836          & 0.2885          & 0.1971          
 GUESS      & 0.0138          & 0.3024          & 0.1446          & 0.0347          


First, the bi-directional image-word model is still the best model, which is consistent with the results we achieved in DAQUAR. Second, the model has reasonable ability answering object-type questions, as the bi-directional model wins 7\






    [width=\textwidth, height=.7\textwidth] formation1.jpg 
    Q683: how many military jet fighter is flying in formation alongside a 1 military propeller pilot ?
    2-IMGWD: four (0.402)
    Groud truth: one

    [width=\textwidth, height=.7\textwidth] formation2.jpg 
    Q7746: how many airplanes are flying up in the air in formation ?
    2-IMGWD: four (0.334)
    Groud truth: four

    [width=\textwidth, height=.7\textwidth] formation3.jpg 
    Q8473: how many jets in the sky in a flying formation ?
    2-IMGWD: four (0.329)
    Groud truth: eight

    [width=\textwidth, height=.7\textwidth] giraffe1.jpg 
    Q8325: how many giraffes are partially hidden in brush and sticking their necks out ?
    2-IMGWD: three (0.302)
    Groud truth: two

    [width=\textwidth, height=.7\textwidth] giraffe2.jpg 
    how many giraffes in grassy field next to trees ?
    2-IMGWD: three (0.361)
    Groud truth: three

    [width=\textwidth, height=.7\textwidth] giraffe3.jpg 
    Q13103: how many giraffes standing together and eating near some trees ?
    2-IMGWD: three (0.317)
    Groud truth: five

 Counting ability of image-word model: output is strongly dependent on the type of object described in the question. 

    [width=\textwidth, height=.7\textwidth] two1.jpg 
    Q44: an open food container box with how many unknown food items ?
    2-IMGWD: two (0.258)
    Groud truth: four

    [width=\textwidth, height=.7\textwidth] two2.jpg 
    Q47: the beautiful dessert waiting to be shared by how many people ?
    2-IMGWD: two (0.450)
    Groud truth: two

    [width=\textwidth, height=.7\textwidth] two3.jpg 
    Q1016: how many motor cycle racers posing on their parked bikes ?
    2-IMGWD: two (0.460)
    Groud truth: three

 Counting ability of image-word model: model outputs ``two'' when it is not certain about the object. 

    [width=\textwidth, height=.7\textwidth] banana1.jpg 
    Q10371: what is the color of the bananas ?
    2-IMGWD: green (0.488)
    Groud truth: green

    [width=\textwidth, height=.7\textwidth] banana2.jpg 
    Q16954: what is the color of the bananas ?
    2-IMGWD: green (0.488)
    Groud truth: yellow

    [width=\textwidth, height=.7\textwidth] banana3.jpg 
    Q1016: how many motor cycle racers posing on their parked bikes ?
    2-IMGWD: green (0.488)
    Groud truth: yellow

    [width=\textwidth, height=.7\textwidth] cat1.jpg 
    Q18455: what is the color of the cat ?
    2-IMGWD: black (0.227)
    Groud truth: yellow

    [width=\textwidth, height=.7\textwidth] cat2.jpg 
    Q19096: what is the color of the cat ?
    2-IMGWD: black (0.227)
    Groud truth: black

    [width=\textwidth, height=.7\textwidth] cat3.jpg 
    Q19096: what is the color of the cat ?
    2-IMGWD: black (0.227)
    Groud truth: grey

 Color recognition ability of image-word model: output is strongly dependent on the type of object described in the question. In this case ``green'' banana is more ``worth mention'' in image descriptions. Even worse, since the questions are the same, the output probability are the same regardless of the images. The output class is consistent with the blind model. 

    [width=\textwidth, height=.7\textwidth] motorcycle1.jpg 
    Q5141: what parked on the side of the road ?
    2-IMGWD: motorcycle (0.226)
    BLIND: truck (0.110)
    Groud truth: motorcycle

    [width=\textwidth, height=.7\textwidth] onbed.jpg 
    Q5017: what is sitting on top of someones bed ?
    2-IMGWD: computer (0.303)
    BLIND: cat (0.229)
    Groud truth: bicycle

    [width=\textwidth, height=.7\textwidth] birdcat.jpg 
    Q2472: what is sitting on the handle bar of a bicycle ?
    2-IMGWD: cat (0.460)
    BLIND: cat (0.256)
    Groud truth: bird

 Object recognition ability of image-word model: thanks to the image features from CNN, it wins over the blind model by 7\





































