\chapter{Discussion}

In the previous chapter we presented two models, the single directional image-word model and the bi-directional model with some variations. We tested these models on two datasets, DAQUAR \cite{malinowski14b} and COCO-QA which is automatically generated from MS-COCO image description dataset. We showed that the model has demonstrated non-trivial ability as it wins over a number of baselines, particularly the previous attempt on DAQUAR dataset \cite{malinowski14b}. By replacing the actual image with random numbers, we further evaluated the usefulness of the image features in these models. In this section, we will break down our discussion into four parts: model selection, dataset selection, semantic knowledge, and visual knowledge.

\section{Model selection}
\subsection{LSTM sentence model}
The LSTM is shown to be a very effective sentence model. It not only adds up the word semantics as does in the bag-of-word (BOW) model, but also dynamically weights every word when input to the memory cell without any handcrafted heuristics (e.g. TF-IDF \cite{salton88}). The image-word model always outputs the correct raw class of answer in the top 10 answers: ``what'' corresponds to an object; ``who'' corresponds to a person, ``how many'' corresponds to a number, and ``what is the colour'' corresponds to a colour. Note that ``how many'', ``what'', and ``who'' not always appear in the beginning of the sentence as some image descriptions cannot perform wh-fronting algorithm. Although this might seem to be a simple task, the BOW model in fact did not perform very well on it. Compared to BOW, the LSTM distinguishes different orderings of the words. For example, BOW cannot distinguish questions like ``what is on the red chair beside the table ?'' and ``what is on the red table beside the chair ?''. Based on the results presented in the last chapter, we argue that LSTM is an excellent model for sentence-level embedding.

\subsection{Word embedding}
In all of our experiments, we either take a word2vec embedding trained from Google News \cite{mikolov13} or use a frozen randomly initialized word embedding. We did not experiment weight-update on the randomly initialized word embedding because given the amount of data in DAQUAR, it is very easy to overfit in the training set. However, with more data as in the case of COCO-QA, it may be worthwhile to train a task-specific word embedding. Based on the experimental results, we observed that word2vec embedding performs better than random initialized embedding, especially on the COCO-QA dataset, where there are more variety of object classes. This shows the effectiveness of pre-trained word embedding.

\subsection{Single direction or bi-direction}
In all the experiments, the bi-directional model wins over the single directional model in terms of both plain accurarcy and WUPS. However, the gain is usually under 1\%. If we take into consideration that the bi-directional model takes two times the training time, then the benefit of bi-directional model is not very significant. 

\subsection{Softmax cross entropy or ranking loss}
As discussed in Section~\ref{sec:daquar_results}, although the ranking loss makes the training faster, the results does not outperform cross entropy because the model overfits the training set too quickly. It is also questionable whether it makes sense to use generic word embedding trained from Google News as the target vector. Both softmax cross entropy and ranking loss are two effective loss function for training such objectives, and will be tested as one of our hyperparameters in the future work.

\section{Dataset selection}
DAQUAR dataset is the first dataset on the image question-answering tasks. It is human generated, so all the questions are understandable and well written. We noted in Section~\ref{background_daquar} that some questions are too hard to answer, as human can only achieve 60\% accuracy on this dataset. Moreover, with only 1500 images and 7000 questions, this dataset is too small for deep learning approaches. Lastly, the fact that a simple GUESS baseline can perform very well suggests that we need to have more even answer distribution so that the mode takes less proportion of the entire distribution.

We then created a dataset called COCO-QA that aims to directly address to the three issues mention above. All questions are generated from image description, so the questions will mostly focus on salient objects in the image instead on small details. It contains 20 times more questions and 60 times more images, which is more suitable to deep learning approaches. With a specially designed ``mode damper'' that reduces the most common answers, the GUESS baseline can only get 3.47\% accuracy on the full dataset. On the other hand, it is hard to evaluate the quality and the difficulty of the questions. More future improvement will be discussed in the next chapter.

The results from Table~\ref{tab:cocoqa_results} actually show that our model scales to a large number of object types and answer classes. Both single directional and bi-directional models achieve over 30\% accuracy on the full COCO-QA dataset, which has 794 possible answer classes and over 13K vocabularies. 

Currently, despite the large number of questions and images, the types of questions are quite limited: they are object, number, and colour. However, the fact that our model does not perform very well on counting and colour questions warns us whether it is meaningful to extend to even broader ranges of questions too quickly at this stage. Although counting objects and colour recognition can be addressed using problem-specific computer vision techniques, to work towards a generic QA framework, it is worthwhile to investigate generic machine learning models that perform well on those specific tasks first before we expand to a larger variety of questions.

\section{Visual and semantic knowledge}
Based on the results shown in Table~\ref{tab:daquar_results} and Table~\ref{tab:cocoqa_results}, the major gain of our model is achieved through a better language model. In Figure~\ref{fig:cocoqa_number} and Figure~\ref{fig:cocoqa_colour}, same objects described in the questions are always assigned with same colours and numbers. Only in rare occasions such as Figure~\ref{fig:cocoqa_object} where the model does not have sufficient hints from the questions, the image features help the model get the correct answer. Even worse, exactly same questions will output very similar answer probabilities. We conclude that in these cases, our model is not using the image signals at all, which is possible when the LSTM ``forgets'' the image features of the first timestep over the time through the ``forget gate'', and for bi-directional models, the image features at the last timestep are rejected by the ``input gate''. It is also possible that because we used the last hidden layer of the CNN, same object classes already have very similar and problem-specific feature vector representations, so the image features are too similar to be used for asking other types of questions. The similarity in image feature vectors may explain the differences in terms of plain accuracy on counting and colour recognition between the blind model and the image model in Table~\ref{tab:cocoqa_acc_breakdown}, because the image models can simply predict the mode conditioned on the words appeared in the question without taking in too much noise from the image features. It will be future work for us to explore other alternatives to represent images.